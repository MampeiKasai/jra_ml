{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5c05c637",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import csv\n",
    "from typing import Any\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from fake_useragent import UserAgent\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "html_dir_path = '/Users/eee/python/jra_ml/data/html'\n",
    "csv_dir_path = '/Users/eee/python/jra_ml/data/table'\n",
    "\n",
    "class Scrape:\n",
    "    def __init__(self, html_dir_path, csv_dir_path):\n",
    "        self.hfm = HtmlFileManager(html_dir_path)\n",
    "        self.cfm = CsvFileManager(csv_dir_path)\n",
    "        self.url = 'https://db.netkeiba.com/'\n",
    "        self.response = ''\n",
    "        self.bs = ''\n",
    "        \n",
    "        \n",
    "    def set_bs(self, bs):\n",
    "        self.bs = bs\n",
    "        \n",
    "        \n",
    "    def set_url(self, url):\n",
    "        self.url = url\n",
    "        \n",
    "        \n",
    "    def get_response(self) -> Any:\n",
    "        ua = UserAgent()\n",
    "        header = {'user-agent': ua.chrome}\n",
    "        \n",
    "#        print('Getting server response...')\n",
    "        \n",
    "        self.response = requests.get(self.url, 'lxml', headers=header)\n",
    "        self.response.encoding = self.response.apparent_encoding\n",
    "        self.bs = BeautifulSoup(self.response.content, 'lxml')\n",
    "        \n",
    "        time.sleep(1)\n",
    "        \n",
    "    \n",
    "    # カレンダーから各日のレースリスト IDをスクレイピング\n",
    "    def scrape_race_schedule(self, url):\n",
    "#        print(f'    Scraping race_list from {url}')\n",
    "        \n",
    "        self.set_url(url)\n",
    "        self.hfm.convert_html_file_name(self.url)\n",
    "        if self.hfm.file_exists():\n",
    "            self.set_bs(self.hfm.load_html())\n",
    "        else:\n",
    "            self.get_response()\n",
    "            self.hfm.save_html(self.bs)            \n",
    "        \n",
    "        race_list_path = []\n",
    "        race_cal_tag = self.bs.find_all(href=re.compile('\\?pid=race_top&'))\n",
    "        \n",
    "        pre_month_date_url = ''\n",
    "        if race_cal_tag[1].get('title') == '前へ':\n",
    "            pre_month_path = race_cal_tag[1].get('href')\n",
    "            pre_month_date_url = base_url + pre_month_path        \n",
    "\n",
    "        dc = re.compile('^[0-9]')\n",
    "        race_list_tag = self.bs.find_all(href=re.compile('/race/list/'))\n",
    "        for rlt in race_list_tag:\n",
    "            if dc.match(rlt.contents[0]):\n",
    "                race_list_path.append(rlt.get('href'))\n",
    "                \n",
    "        # 無料会員で見れる範囲に制限。\n",
    "#        if pre_month_date_url == 'https://db.netkeiba.com/?pid=race_top&date=20201205'\n",
    "#        if pre_month_date_url == 'https://db.netkeiba.com/?pid=race_top&date=20150201':\n",
    "        #if pre_month_date_url == 'https://db.netkeiba.com/?pid=race_top&date=20080503':\n",
    "        if pre_month_date_url == 'https://db.netkeiba.com/?pid=race_top&date=20000506':\n",
    "    \n",
    "            return race_list_path\n",
    "\n",
    "        if pre_month_date_url != '':\n",
    "            temp_list = self.scrape_race_schedule(pre_month_date_url)\n",
    "            race_list_path.extend(temp_list)\n",
    "            return race_list_path\n",
    "        else:\n",
    "            return race_list_path\n",
    "        \n",
    "    # 各レースのURL IDをスクレイピング\n",
    "    def scrape_race_id(self, url: str) -> list:\n",
    "#        print(f'    Scraping race_id from {url}')\n",
    "        \n",
    "        self.set_url(url)\n",
    "        self.hfm.convert_html_file_name(self.url)\n",
    "        if self.hfm.file_exists():\n",
    "            self.set_bs(self.hfm.load_html())\n",
    "        else:\n",
    "            self.get_response()\n",
    "            self.hfm.save_html(self.bs)\n",
    "        \n",
    "        race_id_list = []\n",
    "        race_list_tag = self.bs.find_all(class_=re.compile('race_top_data_info'))\n",
    "        \n",
    "        for rlt in race_list_tag:\n",
    "            race_id = rlt.contents[3].contents[1].get('href')\n",
    "            race_type = rlt.contents[3].contents[4].contents[0]\n",
    "            if '障' not in race_type:\n",
    "                race_id_list.append(race_id)\n",
    "                \n",
    "#        print(f'    ---- # of race: {len(race_id_list)}.')\n",
    "                \n",
    "        return race_id_list\n",
    "    \n",
    "    def scrape_race_info(self, url: str, race_id: str):\n",
    "#        print(f'    Scraping race_info from {url}')\n",
    "        \n",
    "        self.set_url(url)\n",
    "        self.hfm.convert_html_file_name(self.url)\n",
    "        if self.hfm.file_exists():\n",
    "            self.set_bs(self.hfm.load_html())\n",
    "        else:\n",
    "            self.get_response()\n",
    "            self.hfm.save_html(self.bs)\n",
    "        \n",
    "        course_info_list = []\n",
    "        horse_id_list = []        \n",
    "\n",
    "        course_info_list = s.scrape_course_info(race_id)\n",
    "\n",
    "        horse_id_list = s.scrape_horse_id()\n",
    "\n",
    "        race_result_df = s.scrape_race_result()\n",
    "        race_result_df = race_result_df.assign(race_id=race_id)\n",
    "        horse_id_list = [[hil[0].split('/')[2], hil[1]] for hil in horse_id_list]\n",
    "        race_result_df = race_result_df.assign(horse_id=[name[0] for name in horse_id_list])\n",
    "        \n",
    "        return course_info_list, horse_id_list, race_result_df\n",
    "    \n",
    "    \n",
    "    def scrape_course_info(self, race_id):\n",
    "#        print('    ---- Scraping course_info.')\n",
    "        race_info_list = []\n",
    "\n",
    "        course_info_tag = self.bs.find_all('diary_snap_cut')[0]\n",
    "        temp_list = str(course_info_tag.contents[1]).split('\\xa0')\n",
    "        course_info_list = [t for t in temp_list if '/' !=  t]\n",
    "\n",
    "        if '芝' in course_info_list[0]:\n",
    "            race_type = '芝'\n",
    "            split_str = '芝'\n",
    "        else:\n",
    "            race_type = 'ダート'\n",
    "            split_str = 'ダ'\n",
    "\n",
    "        if '右' in course_info_list[0]:\n",
    "            around = '右'\n",
    "            split_str = '右'\n",
    "        elif '左' in course_info_list[0]:\n",
    "            around = '左'\n",
    "            split_str = '左'\n",
    "        elif '直線' in course_info_list[0]:\n",
    "            around = '直線'\n",
    "            split_str = '直線'\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "\n",
    "        if '外・内' in course_info_list[0]:\n",
    "            course_position = '外・内'\n",
    "            split_str = '外・内'\n",
    "        elif '外-内' in course_info_list[0]:\n",
    "            course_position = '外-内'\n",
    "            split_str = '外-内'\n",
    "        elif '内2周' in course_info_list[0]:\n",
    "            course_position = '内'\n",
    "            split_str = '内2周'\n",
    "        elif '外2周' in course_info_list[0]:\n",
    "            course_position = '外'\n",
    "            split_str = '外2周'\n",
    "        elif '外' in course_info_list[0]:\n",
    "            course_position = '外'\n",
    "            split_str = '外'\n",
    "        elif '内' in course_info_list[0]:\n",
    "            course_position = '内'\n",
    "            split_str = '内'\n",
    "        else:\n",
    "            course_position = 'N/A'        \n",
    "\n",
    "        patt = '/race/' + race_id\n",
    "        course_name_tag = self.bs.find(href=re.compile(patt))\n",
    "        course_name = course_name_tag.contents[0]\n",
    "\n",
    "        course_len = int(course_info_list[0].split(split_str)[1].rstrip('m'))\n",
    "        weather = course_info_list[1].split(':')[1]\n",
    "        ground_state = course_info_list[2].split(':')[1]\n",
    "\n",
    "        race_date_class_tag = self.bs.find_all('p')[4]\n",
    "    #    race_date =  race_date_class_tag.contents[0].split(' ')[0]\n",
    "    #    race_class = race_date_class_tag.contents[0].split(' ')[2].replace('\\xa0', '')\n",
    "        race_date = race_date_class_tag.contents[0].split(' ')[0]\n",
    "        race_class = race_date_class_tag.contents[0].split(' ')[2]\n",
    "        race_class = race_class.split('\\xa0')[0]\n",
    "\n",
    "\n",
    "        race_info_list = [race_id, course_name, course_len, around, course_position, weather, race_type, ground_state, race_date, race_class]\n",
    "        return race_info_list\n",
    "    \n",
    "    \n",
    "    def scrape_horse_id(self):\n",
    "#        print('    ---- Scraping horse_id.')\n",
    "        \n",
    "        horse_id_list = []\n",
    "        \n",
    "        horse_id_tag = self.bs.find_all(href=re.compile('^/horse'))\n",
    "        horse_id_list = [[hit.get('href'), hit.contents[0]] for hit in horse_id_tag]\n",
    "            \n",
    "        return horse_id_list\n",
    "\n",
    "\n",
    "    \n",
    "    def scrape_race_result(self):\n",
    " #       print('    ---- Scraping race_result.')\n",
    "\n",
    "        html_path = os.path.join(self.hfm.dir_path, self.hfm.file_name)\n",
    "        temp_df = pd.read_html(html_path, header=0)\n",
    "\n",
    "        race_result_df = temp_df[0]\n",
    "        \n",
    "        return race_result_df\n",
    "\n",
    "\n",
    "\n",
    "    def scrape_horse_pedigree(self, url: str, horse_id_name: list):\n",
    "#        print(f'    -------- Scraping horse_pedigree of {horse_id_name[1]} from {url}')    \n",
    "        \n",
    "        self.set_url(url)\n",
    "        self.hfm.convert_html_file_name(self.url)\n",
    "        if self.hfm.file_exists():\n",
    "            self.set_bs(self.hfm.load_html())\n",
    "        else:\n",
    "            self.get_response()\n",
    "            self.hfm.save_html(self.bs)\n",
    "        \n",
    "\n",
    "\n",
    "        horse_pedigree_tag = self.bs.find_all(href=re.compile('/horse/[0-9]'))\n",
    "        horse_pedigree_tag = [pt for pt in horse_pedigree_tag if not pt.get('title')]\n",
    "        \n",
    "        # initialize\n",
    "        horse_pedigree_list = horse_id_name\n",
    "\n",
    "        # add data to output list\n",
    "        for hpt in horse_pedigree_tag:\n",
    "            temp_con = str(hpt.contents[0])\n",
    "            temp_con = temp_con.replace('<span class=\"red\">', '')\n",
    "            temp_con = temp_con.replace('</span', '')\n",
    "            temp_con = temp_con.replace('\\n', '')\n",
    "            temp_con = temp_con.replace('>', '')\n",
    "\n",
    "            horse_pedigree_list.append(temp_con+'('+hpt.get('href').split('/')[2]+')')\n",
    "            \n",
    "        return horse_pedigree_list\n",
    "\n",
    "    def scrape_horse_result(self, url: str, horse_id_name: list):\n",
    "#        print(f'    -------- Scraping horse_result of {horse_id_name[1]} from {url}')\n",
    "  \n",
    "        self.set_url(url)\n",
    "        self.hfm.convert_html_file_name(self.url)\n",
    "        if self.hfm.file_exists():\n",
    "            pass\n",
    "        else:\n",
    "            self.get_response()\n",
    "            self.hfm.save_html(self.bs)\n",
    "\n",
    "        html_path = os.path.join(self.hfm.dir_path, self.hfm.file_name)\n",
    "        temp_df = pd.read_html(html_path, header=0)\n",
    "            \n",
    "        horse_result_df = temp_df[0]\n",
    "        horse_result_df = horse_result_df.assign(horse_id=horse_id_name[0])\n",
    "\n",
    "        return horse_result_df\n",
    "\n",
    "class FileManager(object):\n",
    "    def __init__(self, dir_path):\n",
    "        self.dir_path = dir_path\n",
    "        self.file_name = ''\n",
    "        \n",
    "    def set_dir_pat(self, dir_path):\n",
    "        self.dir_path = dir_path\n",
    "    \n",
    "    def set_file_name(self, file_name):\n",
    "        self.file_name = file_name\n",
    "\n",
    "    def file_exists(self) -> bool:\n",
    "        file_list = os.listdir(self.dir_path)\n",
    "        return True if self.file_name in file_list else False\n",
    "            \n",
    "class HtmlFileManager(FileManager):\n",
    "    def __init__(self, dir_path):\n",
    "        super().__init__(dir_path)\n",
    "        \n",
    "    def load_html(self) -> Any:\n",
    "        file_path = os.path.join(self.dir_path, self.file_name)\n",
    "        bs = BeautifulSoup(open(file_path), 'lxml')        \n",
    "        return bs\n",
    "\n",
    "    def save_html(self, bs):\n",
    "        file_path = os.path.join(self.dir_path, self.file_name)\n",
    "#        response.encoding = response.apparent_encoding\n",
    "        with open(file_path, 'w') as f:\n",
    "            f.write(str(bs))\n",
    "            \n",
    "    def convert_html_file_name(self, url) -> str:\n",
    "        patt_dict = {\n",
    "            'race_cal_': '\\?pid=race_top',\n",
    "            'race_list_': 'race/list/',\n",
    "            'race_info_': 'race/[0-9]',\n",
    "            'horse_result_': 'horse/result/',\n",
    "            'horse_pedigree_': 'horse/ped/'\n",
    "        }\n",
    "        \n",
    "        url = url.replace('https://db.netkeiba.com/', '')\n",
    "        html_name = ''\n",
    "        for k, v in patt_dict.items():\n",
    "            if re.search(v, url):\n",
    "                url_id = re.sub(r\"\\D\", \"\", url)\n",
    "                html_name = k + url_id + '.html'\n",
    "                self.file_name = html_name\n",
    "                break        \n",
    "\n",
    "            \n",
    "class CsvFileManager(FileManager):\n",
    "    def __init__(self, dir_path):\n",
    "        super().__init__(dir_path)\n",
    "        \n",
    "    def make_csvfile(self, column):\n",
    "        file_path = os.path.join(self.dir_path, self.file_name)\n",
    "        with open(file_path, 'w', newline='') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow(column)    \n",
    "\n",
    "    def write_list_to_csv(self, data):\n",
    "        file_path = os.path.join(self.dir_path, self.file_name)\n",
    "        with open(file_path, 'a') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow(data)\n",
    "            \n",
    "    def write_df_to_csv(self, data: Any, index_name: str):\n",
    "        file_path = os.path.join(self.dir_path, self.file_name)\n",
    "        data.set_index(index_name).to_csv(file_path, mode='a', header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdfb91e7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start scraping!!!\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    base_url = 'https://db.netkeiba.com'\n",
    "#    race_top_url = 'https://db.netkeiba.com/?pid=race_top'\n",
    "    race_top_url = 'https://db.netkeiba.com/?pid=race_top&date=20080105'\n",
    "#    race_top_url = 'https://db.netkeiba.com/?pid=race_top&date=20080202'  \n",
    "    # table column name\n",
    "    race_result_column = ['rae_id', '着順', '枠番', '馬番', '馬名', '性齢', '斤量', '騎手', 'タイム', '着差', '単勝', '人気', '馬体重', '調教師', 'horse_id']\n",
    "    course_info_column = ['race_id', 'coures_name', 'course_len', 'around', 'course_position', 'weather', 'race_type', 'ground_state', 'date' , 'race_class']\n",
    "    horse_pedigree_column = ['horse_id', 'horse_name', \n",
    "            'f_1_1', 'f_2_1', 'f_3_1', 'f_4_1', 'f_5_1',\n",
    "            'm_5_1', 'm_4_1', 'f_5_2', 'm_5_2', 'm_3_1',\n",
    "            'f_4_2', 'f_5_3', 'm_5_3', 'm_4_2', 'f_5_4',\n",
    "            'm_5_4', 'm_2_1', 'f_3_2', 'f_4_3', 'f_5_5',\n",
    "            'm_5_5', 'm_4_3', 'f_5_6', 'm_5_6', 'm_3_2',\n",
    "            'f_4_4', 'f_5_7', 'm_5_7', 'm_4_4', 'f_5_8',\n",
    "            'm_5_8', 'm_1_1', 'f_2_2', 'f_3_3', 'f_4_5',\n",
    "            'f_5_9', 'm_5_9', 'm_4_5', 'f_5_10', 'm_5_10',\n",
    "            'm_3_3', 'f_4_6', 'f_5_11', 'm_5_11', 'm_4_6',\n",
    "            'f_5_12', 'm_5_12', 'm_2_2', 'f_3_4', 'f_4_7',\n",
    "            'f_5_13', 'm_5_13', 'm_4_7', 'f_5_14', 'm_5_14',\n",
    "            'm_3_4', 'f_4_8', 'f_5_15', 'm_5_15', 'm_4_8',\n",
    "            'f_5_16', 'm_5_16'\n",
    "            ]\n",
    "    horse_result_column = ['horse_id', '日付', '開催', '天気', 'R', 'レース名', '映像',\n",
    "            '頭数', '枠番', '馬番', 'オッズ', '人気', '着順', '騎手', '斤量',\n",
    "            '距離', '馬場', '馬場指数',  'タイム', '着差', 'ﾀｲﾑ指数', '通過',\n",
    "            'ペース', '上り', '馬体重', '厩舎ｺﾒﾝﾄ', '備考', '勝ち馬(2着馬)', '賞金'\n",
    "            ]\n",
    "\n",
    "    # table file name\n",
    "    cwd_path = os.getcwd()\n",
    "    course_info_file = 'course_info.csv'\n",
    "    race_result_file = 'race_result.csv'\n",
    "    horse_pedigree_file = 'horse_pedigree.csv'\n",
    "    horse_result_file = 'horse_result.csv'\n",
    "    \n",
    "    s = Scrape(html_dir_path, csv_dir_path)\n",
    "    # make table file\n",
    "    s.cfm.set_file_name(course_info_file)\n",
    "    s.cfm.make_csvfile(course_info_column)\n",
    "    \n",
    "    s.cfm.set_file_name(race_result_file)\n",
    "    s.cfm.make_csvfile(race_result_column)\n",
    "\n",
    "    s.cfm.set_file_name(horse_pedigree_file)\n",
    "    s.cfm.make_csvfile(horse_pedigree_column)\n",
    "    \n",
    "    s.cfm.set_file_name(horse_result_file)\n",
    "    s.cfm.make_csvfile(horse_result_column)\n",
    "    \n",
    "\n",
    "    \n",
    "    print(\"Start scraping!!!\")\n",
    "    \n",
    "    # Get race schedule.\n",
    "    race_list_path = s.scrape_race_schedule(race_top_url)\n",
    "    \n",
    "    # Get race id(only dirt or turf) per day.\n",
    "    race_id_list = []\n",
    "    for rlp in race_list_path:\n",
    "        race_list_url = base_url + rlp\n",
    "        temp_list = s.scrape_race_id(race_list_url)\n",
    "        race_id_list.extend(temp_list)\n",
    "\n",
    "    # Get race detail (course_info, race_result) per race.\n",
    "    for ril in race_id_list:\n",
    "        race_id_url = base_url + ril\n",
    "        race_id = ril.split('/')[2]\n",
    "        course_info_list, horse_id_list, race_result_df = s.scrape_race_info(race_id_url, race_id)      \n",
    "\n",
    "        s.cfm.set_file_name(course_info_file)\n",
    "        s.cfm.write_list_to_csv(course_info_list)\n",
    "        \n",
    "        s.cfm.set_file_name(race_result_file)\n",
    "        s.cfm.write_df_to_csv(race_result_df, 'race_id')\n",
    "        \n",
    "        # Get horse info(horse_pedigree, horse_result) per horse.\n",
    "        scraped_data = set()\n",
    "        for hil in horse_id_list:\n",
    "            if hil[0] not in scraped_data:\n",
    "                horse_pedigree_url = base_url + '/horse/ped/' + hil[0]\n",
    "                horse_pedigree_list = s.scrape_horse_pedigree(horse_pedigree_url, hil)\n",
    "                \n",
    "                s.cfm.set_file_name(horse_pedigree_file)\n",
    "                s.cfm.write_list_to_csv(horse_pedigree_list)\n",
    "                \n",
    "                horse_result_url = base_url + '/horse/result/' + hil[0]\n",
    "                horse_result_df = s.scrape_horse_result(horse_result_url, hil)\n",
    "                \n",
    "                s.cfm.set_file_name(horse_result_file)\n",
    "                s.cfm.write_df_to_csv(horse_result_df, 'horse_id')\n",
    "        \n",
    "                scraped_data.add(hil[0])\n",
    "            \n",
    "    print(\"End scraping!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efa4cf3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52646d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
